%
% File eacl2014.tex
%
% Contact g.bouma@rug.nl yannick.parmentier@univ-orleans.fr
%
% Based on the instruction file for ACL 2013 
% which in turns was based on the instruction files for previous 
% ACL and EACL conferences

%% Based on the instruction file for EACL 2006 by Eneko Agirre and Sergi Balari
%% and that of ACL 2008 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amssymb}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to

\title{Learning Bi-lingual Word Representations using Uby \\a Large-Scale Unified
Lexical Semantic Resource}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Word representations and specially word feautres induced by distributed models
  are shown to be able to boost the performance of various NLP tasks such as
  Word Sense Disambiguation, Named Entity Recognition, Parsing,\ldots
  Previously a model have been proposed  to learn
  representation for entities of a structured knowldege base such as WordNet.
  
  Here in this paper, we follow the previous work and extend their idea by
  incorporating multiple resources  in order to induce richer representations jointly for two different languages.  
  We have evaluated both monolingual  and bilingual embeddings on four different gold dataset
for word-pair similarity task and shown that bilingual embeddings perform similarly or better than monolingual embeddings. 
For example on one of datasets, bilingual embeddings is 10 percent more correlated (Pearson correlation) to human judgements  than 
monolingual embeddings of the same model and up to 40 percent more than the other models.

\end{abstract}


\section{Introduction}
% das: removed reference to PostScript

In a large number of machine learning methods and their application to computational linguistics
 \emph{feature engineering} or extracting informative features is a crucial part
 and it is done mostly manually. \emph{ Representation Learning} is an umberella term
for a family of unsupervised methods to learn features from data to decrease the manual labour. Most of recent
works related to this idea focus on inducing word
representations. \emph{Word representation} or \emph{Word embedding} ''is a mathematical object, usually a
vector, which each dimension in this vector represents a grammatical or
semantical feature to identify this word and is induced automatically from data''
\cite{Turian2010b}. Recently, it has been shown in \cite{Turian2010b} and \cite{Collobert2011} that using
 induced word representations can be helpful to improve state-of-the-art methods in 
variouse standard tasks. While their word embeddings are induced for a single
language, Klementiev et al.
[Inducing Crosslingual Distributed Representations of Words ] have a model which
learns cross-lingual representations and is shown to have superior performance
for text classification task over strong baselines. In contrast to previous similar works which word
embeddings learnt from a corpus, Bordes et al. proposed a method
\cite{Bordes2011} to learn distributed representations from multi-relational knowledge bases(KB) such as WordNet 
and Freebase. 
They encode information in KBs as binary relations between entities which each relation
 is instantiated from a set of relation types. Since we are following their methodology, 
 a description of their work is presented in ~\ref{rel-work:structured-embedding}.
 
 This paper is motivated by two questions. The first question is that how can we extend the framework in (Bordes2011)
  to induce crosslingual word representations to benefit from aggregation of information in two 
  different resource in two (or more) different languages.
  The second question is how much informative are these learned features? More specifically, we inspect the degree of ability of 
  these embeddings and also embeddings from other methods to capture semantic similarity between words.
 
 Regarding the first question, In section ?? we introduce our approach to use Bordes framework to learn
 bi-lingual word embeddings from multiple resources. Our contribution is to
 1)infere cross-resource and cross-lingual relations which will enable us to
 share the task of learning embeddings between different resources and languages and 2) encoding this
 information in a way that it can be fed to Bordes model.
 
 Finally, section ?? tries to answer the second question. Therefor we will compare performance of 
 mono-lingual and bi-lingual embeddings induced by different methods as well as ours in 
 word similarity task to investigate the effectiveness of them to
 capture different aspects and features of words meanings(??).
 
 

\section{Representation Learning from Knowledge Bases}

In this section, we will first review a framework proposed in (Bordes2011 and Bordes2012) and then will give  
a short introduction on Uby. In the last part of this section,
 we will show how to use information encoded in or inferred from Uby to induce cross-lingual word representation 
with Bordes et al models.


\subsection{Bordes model for word embedding}
\label{ssect:bordes}
Two major models are proposed in [Bordes2011] and [Bordes2012] to transfer information in Knowledge Bases (KBs)
, encoded as graphs, to continues vector space. The knowledge representation in most of KBs can be expressed
 in form of triples of $(e_{i},r_{k} , e_{j} )$ where $e_{i}$ and $e_{j}$ are $i_{th}$ and $j_{th}$ entities related
 by a binary relation of type $r_{k}$. The purpose of the models is to induce a vector space and associate
  each entity relation to an embedding vector (or a matrix in the case of first model for relations)
   which its dimensions are supposed to reflect a set of informative features of entities and relations.
   
   In the first model, \textbf{structured embeddings}, entities are modeled as \textit{d}-dimensional vectors.
    An associated vector to the $i_{th}$ entity, $e_{i}$, is $E_{i} \in \mathbb{R}^{d}$. Each relation , $r$, 
    is decomposed to two operators each represented as $d \times d$ matrix, $R_{left}$ and $R_{right}$. 
   
   
  
  
   
   





\subsection{Creating of Dataset}
\label{ssec:uby-rel}

 Uby is a unified lexical resource which 
 to our approach for providing cross-resource information. For more information on Uby we will point the readers 
 to this (?) and that(?).
As it is described in the previous section we can relate two senses from two different resources using Uby SenseAxis Alignments.
This is an additional information which can play a role of bridge between two
different tasks to transfer knowledge from one to the another.
Using this new feature we make our WordNet-GermaNet dataset which contains three type of relations 
(1) WordNet relations 
(2) GermaNet relations
(3) Cross-lingual sense relations between WordNet and GermaNet
\\
Example of relations:
\begin{center}
WN-1 \hspace{0.5in}  rel1 \hspace{0.5in}   WN-2\\
GN-1 \hspace{0.5in} rel3 \hspace{0.5in}  GN-2\\
WN-1 \hspace{0.5in} c-rel \hspace{0.5in} GN-2\\
\end{center}

We have also created another version of this dataset but with different granularities, we mapped similar inter-lingual relations to same relations.
This will help to have faster learning phase with roughly similar performance.
For example , in this encoding, [list of relatons] are mapped to [rel1].

We will compare them later together to examine the sensitivity of model to
different granularities of relations. 

Some statistics of data should be shown here.





\section{Evaluation}
\label{sec:eval}

To show the effectiveness of joint learning of features from multiple knowledge bases we suggest 
two experiment setups. In the first schema we follow Bordes et al. ranking task. The goal of this task is
to show how good the structure of knowledge bases are represented through the learned features. 
After we learned the word embeddings from subset(??) of Uby(??),
their ability to reproduce the structure of it will be assessed. On the other hand, the second
setup is investigating on this question that if the learned word embeddings from multiple resources
are able to improve the performance of monolingual embeddings in a standard NLP
task, here word-pair similarity or not.
In this setup we will look to contribution of the learned features in predicting similarity of words.

\subsection{Intrinstic Evaluation}
\label{ssec:intrinsic}

Bordes et al. define a ranking task where for each triplet $(e_{l} , r, e_{r} ) $ in trianing and test set,
     $e_{l}$ will be removed and all the entities will be ranked by 
     using 1-norm rank function  ( equation ??? decomposing equation). A higher rank of $e_{l}$ (lower number)
     reflects the better quality of learned representations. Additionally they have compared this result to
     another ranking schema using density estimation . 
     In this schema, for each word embedding $e$ the density of $(e , r, e_{r} )$ will be computed
     ( as it is described in our section???) and triplets will be sorted by their estimated probability 
     (probability terms ??). Since we are using larger sets of triplets, instead of ranking
     all the training instances
     we sample randomly from each training dataset with size of 20\% of the original dataset(??) then
     we test our models on these sampled training instances and all the instances from test set. Bordes et al. have followed
     a similar  approach for ranking their embeddings on their biggest dataset. We re-run  their related experiments to make 
     the comparison to our embeddings meaningful. Table (??) shows the results.

\begin{table*}[ht]
\caption{Ranking Performance for Non-mapped Relations } % title name of the table
\centering % centering table
\begin{tabular}{l c c c c c c} % creating 10 columns
\hline\hline % inserting double-line
 Dataset & \#dimension & \#relations & \#entities &  & Micro & Macro
\\ [0.5ex] 
\hline % inserts single-line

 & & &  & lhs & 82.08 & 73.11 \\[-1ex]
  & & &  & rhs & 81.22 & 72.36 \\[-1ex]
\raisebox{1.5ex}{GermaNet} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{16}& \raisebox{0.5ex}{64025}&global
&  81.65 & 72.74 \\[1ex]

 & & &  & lhs & 81.76 & 85.79 \\[-1ex]
  & & &  & rhs & 81.96 & 85.49 \\[-1ex]
\raisebox{1.5ex}{WordNet} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{23}& \raisebox{0.5ex}{148976}& global
& 81.86 & 85.63 \\[1ex]

 & & &  & lhs & 82.50 & 85.09  \\[-1ex]
  & & &  & rhs & 83.16 & 84.46 \\[-1ex]
\raisebox{1.5ex}{WordNet-GermaNet (WN)} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 82.83 & 84.78 \\[1ex]

 & & &  & lhs & 72.12 & 63.63 \\[-1ex]
  & & &  & rhs & 67.78 & 65.77 \\[-1ex]
\raisebox{1.5ex}{WordNet-GermaNet (GN)} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 69.95 & 64.70 \\[1ex]

 
% [1ex] adds vertical space
\hline % inserts single-line
\end{tabular}
\label{tab:PPer}
\end{table*}
\FloatBarrier    
\begin{table*}[ht]
\caption{Ranking Performance for Mapped Relations } % title name of the table
\centering % centering table
\begin{tabular}{l c c c c c c} % creating 10 columns
\hline\hline % inserting double-line
 Dataset & \#dimension & \#relations & \#entities &  & Micro(\%) & Macro(\%)
\\ [0.5ex] 
\hline % inserts single-line

 & & &  & lhs & 82.60 & 68.18 \\[-1ex]
  & & &  & rhs & 81.90 & 68.84 \\[-1ex]
\raisebox{1.5ex}{GermaNet} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{10}& \raisebox{0.5ex}{64025}&global
&  82.25 & 68.51 \\[1ex]

 & & &  & lhs & 83.50 & 83.17 \\[-1ex]
  & & &  & rhs & 84.22 & 83.64 \\[-1ex]
\raisebox{1.5ex}{WordNet} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{19}& \raisebox{0.5ex}{148976}& global
& 83.86 & 83.40 \\[1ex]

 & & &  & lhs & 78.70 & 82.60 \\[-1ex]
  & & &  & rhs & 79.56 & 83.06 \\[-1ex]
\raisebox{1.5ex}{WordNet-GermaNet (WN)} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{24}& \raisebox{0.5ex}{213002}& global
& 79.13 & 82.83 \\[1ex]

 & & &  & lhs & 69.66 & 59.54 \\[-1ex]
  & & &  & rhs & 66.60 & 58.95 \\[-1ex]
\raisebox{1.5ex}{WordNet-GermaNet (GN)} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{24}& \raisebox{0.5ex}{213002}& global
& 68.13 & 59.25 \\[1ex]

 
% [1ex] adds vertical space
\hline % inserts single-line
\end{tabular}
\label{tab:PPer}
\end{table*}
\FloatBarrier    
 We repeat the ranking evaluation with two different embeddings: 
 (1) learned from GermaNet (2) jointly learned from GermaNet-WordNet. The intrinsic evaluation we use here
can't be used to compare the effectivness of these two different embeddings since the evaluation only reflects the difficulty level
of a structure and since these 
 
  Table (??) presents
 the comparison of ranking tasks for mono-lingual and bilingual word embeddings.
     


\subsection{Extrinsic Evaluation}
\label{ssec:extrinsic}
 \label{exp:word-similarity}
 We are interested to further analyze the role of multi-task learning of embeddings for transforming knowledge
 from one resource to the another. In order to examine
 if semantic information from English (WordNet) can be transfered to German (GermaNet) or the other way,
 we compare the embeddings learnt from multiple resources to the embeddings learnt from single resource in word-pair similarity experiments.
 Four datasets of word-pair similarity are used to compare the correlation of predicted similairty of pair of words against human judgments.
 [rubensteinGoodenough], [yangPowers], [millerCharles] and [finkelstein] are datasets that we used to meaure the correlation of similarities
 predicted by the original bordes model (single resource) and our proposed model (multiple resource) to human judgments.
 To measure the similarity between any given wordpair $(w_1 , w_2)$ we find all vectors associated to different senses
 of the given words in our embedding dictionary and compute and find the maximum cosine similarity between two vectors.
 Then for each dataset, both Pearson and Spearman correlation among predicted and gold similarities 
 were calculated which is reported in table \ref{tab:en-wp-sim}. 
 
 \begin{table*}[ht]
\caption{Word-pair Similarity Performance for English } % title name of the table
\centering % centering table
\tabcolsep=0.09cm
\begin{tabular}{cr c c c c c c c} 
\hline\hline % inserting double-line
 Dataset & & WN-SE  & WN-GN-SE & WN-SME-BIL &  WN-GN-SME-BIL & HLBL & Turian & KlementievTitov
\\ [0.5ex] 
\hline % inserts single-line
                                           &  P & 0.488  & 0.571 & 00 & 00 \\[-1ex]
\raisebox{1.5ex}{RubensteinGoodenough65}  &  S & 0.426 & 0.528 & 00 & 00 \\[1ex]

                                    &  P & 0.454 & 0.438 & 00 & 00 \\[-1ex]
\raisebox{1.5ex}{MillerCharles30}  &  S & 0.40 & 0.34 & 00 & 00 \\[1ex]

                                   &  P & 0.194  & 0.177 & 00 & 00 \\[-1ex]
\raisebox{1.5ex}{Finkelstein353}  &  S & 0.137 & 0.128 & 00 & 00 \\[1ex]

                                  &  P & 0.634  & 0.771 & 00 & 00 \\[-1ex]
\raisebox{1.5ex}{YangPowers130}  &  S & 0.598 & 0.770 & 00 & 00 \\[1ex]


\hline % inserts single-line
     
          
 \hline % inserts single-line
\end{tabular}
\label{tab:en-wp-sim}
\end{table*}      

\FloatBarrier    

\begin{table*}[ht]
\caption{Word-pair Similarity Performance for German } % title name of the table
\centering  % centering table
\tabcolsep=0.09cm
\begin{tabular}{cr c c c c c c} % creating 10 columns
\hline\hline % inserting double-line
 Dataset & & GN-SE  & WN-GN-SE & GN-SME-BIL &  WN-GN-SME-BIL & KlementievTitov
\\ [0.5ex] 
\hline % inserts single-line
                                 &  P & -0.022  & 0.112 & 0.058 & 00& 0.118 \\[-1ex]
\raisebox{1.5ex}{wortpaare222}  &  S & -0.100 & 0.225 & 0.230 & 00 & 0.153 \\[1ex]

                                  &  P & 0.865 & 0.984 & -0.443 & 00 & -0.887 \\[-1ex]
\raisebox{1.5ex}{wortpaare30}    &  S & 1.0 & 1.0 & -0.500 & 00 & -1.0 \\[1ex]

                                  &  P & -0.089  & 0.045 & 0.163 & 00& 0.168 \\[-1ex]
\raisebox{1.5ex}{wortpaare350}  &  S & -0.158 & -0.017  &  0.135 & 00 & 0.117 \\[1ex]

                                &  P & 0.800  & 0.558 & -0.572 & 00 & 0.233 \\[-1ex]
\raisebox{1.5ex}{wortpaare65}  &  S & 0.800 & 0.800 & -0.8 & 00 & 0.200 \\[1ex]


\hline % inserts single-line
     
          
 \hline % inserts single-line
\end{tabular}
\label{tab:en-wp-sim}
\end{table*}      
          
\FloatBarrier          
            
As we see in the table \ref{tab:en-wp-sim} in two datasets the performance of learned embeddings from bi-lingual resources
are slightly worse but comparable to the mono-lingual embeddings and in the other two datasets one can observe a significant 
increase of performance of bi-lingual resources over monolingual resources.      


More analysis on why some dataset is good and some is not good. 
\section{Conclusion and Future Work}

Papers that had software and/or dataset submitted for the review
process should also submit it with the camera-ready paper. Besides,
the software and/or dataset should not be anonymous.

Please note that the publications of EACL-2014 will be publicly
available at ACL Anthology (http://aclweb.org/anthology-new/) on April
19th, 2014, one week before the start of the conference. Since some of
the authors may have plans to file patents related to their papers in
the conference, we are reminding authors that April 19th, 2014 may be
considered to be the official publication date, instead of the opening
day of the conference.



% If you use BibTeX with a bib file named eacl2014.bib, 
% you should add the following two lines:
\bibliographystyle{acl}
\bibliography{eacl2014}

% Otherwise you can include your references as follows:
%% \begin{thebibliography}{}

%% \bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
%% Alfred~V. Aho and Jeffrey~D. Ullman.
%% \newblock 1972.
%% \newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
%% \newblock Prentice-{Hall}, Englewood Cliffs, NJ.

%% \bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
%% {American Psychological Association}.
%% \newblock 1983.
%% \newblock {\em Publications Manual}.
%% \newblock American Psychological Association, Washington, DC.

%% \bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
%% {Association for Computing Machinery}.
%% \newblock 1983.
%% \newblock {\em Computing Reviews}, 24(11):503--512.

%% \bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
%% Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
%% \newblock 1981.
%% \newblock Alternation.
%% \newblock {\em Journal of the Association for Computing Machinery},
%%   28(1):114--133.

%% \bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
%% Dan Gusfield.
%% \newblock 1997.
%% \newblock {\em Algorithms on Strings, Trees and Sequences}.
%% \newblock Cambridge University Press, Cambridge, UK.

%% \end{thebibliography}

\end{document}
