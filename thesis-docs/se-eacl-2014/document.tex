%
% File eacl2014.tex
%
% Contact g.bouma@rug.nl yannick.parmentier@univ-orleans.fr
%
% Based on the instruction file for ACL 2013 
% which in turns was based on the instruction files for previous 
% ACL and EACL conferences

%% Based on the instruction file for EACL 2006 by Eneko Agirre and Sergi Balari
%% and that of ACL 2008 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to

\title{Learning Bi-lingual Word Representations using Distributed Neural Models}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Word representations and specially word feautres induced by distributed models
  are shown to be able to boost the performance of various NLP tasks such as: 
  word sense disambiguation, named entity recognition and parsing.
  Previously a model have been proposed  to learn
  representation for entities of a structured lexical resource such as WordNet.
  
  Here in this paper, we follow the previous work and extend their idea by
  incorporating multiple resources  in order to induce richer representations jointly for two different languages.  
  We have evaluated both mono-lingual  and bi-lingual embeddings on four
  different gold datasets for word-pair similarity task and shown that bi-lingual embeddings perform similarly or better than mono-lingual embeddings. 
For example on one of datasets, bi-lingual embeddings is 10 percent(??) more
correlated (Pearson correlation) to human judgements  than mono-lingual embeddings of the same model and up to 40 percent more than the other models.

\end{abstract}


\section{Introduction}
% das: removed reference to PostScript

In a large number of machine learning methods and their application to computational linguistics
 feature engineering or extracting informative features is a crucial part
 and it is done mostly manually. \emph{ Representation Learning} is an umberella term
for a family of unsupervised methods to learn features from data to decrease the manual labour. Most of recent
works related to this idea focus on inducing word
representations. \emph{Word representation} or \emph{Word embedding} ''is a mathematical object, usually a
vector, which each dimension in this vector represents a grammatical or
semantical feature to identify this word and is induced automatically from data''
\cite{Turian2010b}. Recently, it has been shown in \cite{Turian2010b} and \cite{Collobert2011} that using
 induced word representations can be helpful to improve state-of-the-art methods in 
variouse tasks. While these word embeddings are induced for a single
language, Klementiev et al.
[Inducing Crosslingual Distributed Representations of Words ] have a model which
learns cross-lingual representations and is shown to have superior performance
for text classification task over strong baselines. In contrast to previous similar works which word
embeddings learnt from a corpus, Bordes et al. proposed a method
\cite{Bordes2011} to learn distributed representations from multi-relational knowledge bases(KB) like Freebase
or lexical resources like WordNet. 
They encode information in KBs as binary relations between entities which each relation
 is instantiated from a set of relation types available in the KB. Since we are following their methodology, 
 a description of their work is presented in ~\ref{ssec:bordes}.
 
 The main motivation of this paper is to propose a framework to induce crosslingual word representations 
 which benefit from aggregation of information in two 
  different resource in two (or more) different languages. We will show in Section~\ref{ssec:dataset} that using machine learning tool
  developd in (Bordes2011 and Bordes2012) and specific encoding of information we will be able to do so.
 
  In order to evalute our embeddings, we have chosen (??grammar) word pairs similarity task which 
  helps us to investigate on effectiveness of our embeddings to capture different aspects and features of words meanings.
     
  In Section ~\ref{sec:eval} we will also show the comparison between our embeddings and the embeddings 
  induced by other models both for English and German. This paper will be concluded by 
  our analysis of result and models we used, as well as our suggestions for future work.
  
 

\section{Representation Learning from Knowledge Bases}

In this section, we will first review a framework proposed in (Bordes2011 and Bordes2012) and then will
 show how to use information encoded in or inferred from multiple lexicons to induce cross-lingual word representation 
with those models.


\subsection{Bordes model for word embedding}
\label{ssec:bordes}
Two major models are proposed in [Bordes2011] and [Bordes2012] to learn features in continues vector space
 from a Knowledge Bases(KB) which information is usually representated in form of triples of $(e_{i},r_{k} , e_{j} )$ 
 where $e_{i}$ and $e_{j}$ are $i_{th}$ and $j_{th}$ entities related
 by a binary relation of type $r_{k}$. The purpose of the models is to induce a vector space and associate
  each entity or relation to an embedding vector or a matrix.
  The dimensions of such an embedding vector are supposed to reflect a set of informative features of entities and relations.
   
   In the first model, \textbf{structured embeddings(SE)}, entities are modeled as \textit{d}-dimensional vectors.
    An associated vector to the $i_{th}$ entity, $e_{i}$, is $E_{i} \in \mathbb{R}^{d}$. Each relation $r_{k}$  
    is decomposed to two operators each represented as $d \times d$ matrix, $ R_k = (R_{k}^{left}, R_{k}^{right})$. 
    These operators transform the left and right entities to a new space induced by each relation and by using 
    a $p$-norm measure  (L1 norm in this work) they associate a similarity value or a score to each triple. 
    This similarity value is being calculated by
    Equation ~\eqref{eq:sim}. 
    \begin{equation}
    \label{eq:sim}Sim(E_{i}, E_{j}, R) = ||R_{k}^{left}E_{i} - R_{k}^{right}E_{j} ||_{1}
    \end{equation}
    The similarity between transformed entities works as a score to measure the strength of a relation holds between two entities. 
      
    Using the idea of contrastive learning , the model will be trained to increase similarity of 
    embeddings for a positive triple (a triple which exists in the KB) or lowering its rank among other training samples
    and decrease the similarity of embeddings when the relation doesn't hold (negative triple) or raising its rank . 
    For each positive triple, two negative triples will be generated by randomly alternating the right entity or left entity with other entities.
    Inspired from large margin methods a constraint is introdcued on the model that forces 
    negative triples to have lower associated similarity value  than correspondent positive triples by a large margin.
    
    The second model, \textbf{Semantic Matching Energy using Bilinear layers(SME-Bil)}, 
    is using a different represention for relations,weighted bilinear transformation of embeddings 
    and  dot product similarity function instead of L1 norm. 
    In this model, each relation is represented by a \textit{d}-dimensional vector $R_{k}$ same as entities. 
    For triple $(e_{i},r_{k} , e_{j} )$ , the model combines the weighted transformation of each entity embedding with 
    the weighted embedding of relation using element-wise vector product. as it is shown in Equation ~\eqref{eq:bil}.
    \begin{equation}
    \label{eq:bil} E'_{left} = (W_{i} E_{i}) \odot (W_{k} R_{k}) + b_{left}
    \end{equation}
    $W_{i}$ and $W_{k}$ are $d \times d$ weight matrices and $b_{left}$ is a $d$-dimensional bias vector. 
    The same equation holds for transforming the right entity embeddings to $E'_{right}$. Finally, the associated score for the triple
     can be calucated by dot product of $E'_{left}$ and $E'_{right}$ which is shown in Equation ~\ref{eq:dot}.
    
   \begin{equation}
    \label{eq:dot} Sim(E_{i}, E_{j}, R_{k}) = -E'_{left}E'_{right}
   \end{equation}
    Similar constraints to the first model are also applied to this model and 
     both models can be trained by stochahstic gradient descent (SGD) (??)
    
  
  \subsection{Creating of Dataset}
\label{ssec:dataset}

In this part of paper we describe the methodology we followed to encode available information 
in two different lexical resources, WordNet and GermaNet, that makes it possible to learn bi-lingual 
embeddings of word senses in German and English.  
The main idea is to relate two senses from two different resources using cross-lingual sense alignments.
This is an additional information which can play a role of bridge between two
different tasks, learning German embeddings and English embeddings, and can help to transfer knowledge from one to the another.
Using this new feature we make our WordNet-GermaNet dataset which contains three type of relations 
(1) WordNet relations 
(2) GermaNet relations
(3) Cross-lingual sense alignments between WordNet and GermaNet

First two types of relations are directly extracted from WordNet and GermaNet and for the 
cross-lingual relations we used Interlingual Index mappings between WordNet and GermaNet.
\\
Example of relations:
\begin{center}
WN-sense-A \hspace{0.3in}  WN-rel-1    \hspace{0.3in}  WN-sense-B\\
GN-sense-C \hspace{0.3in}  GN-rel-2    \hspace{0.3in}  GN-sense-D\\
WN-sense-A \hspace{0.3in}  ILI-rel-1-2 \hspace{0.3in}  WN-sense-B\\
\end{center}

Left and right entities are WordNet and GermaNet senses and relations are current semantical relations in each of lexicons such as:
 meronymy, holonymy and \ldots.
 
 We have created four different dataset, each divided to train, test and validation separated subsets. Our four datasets are:
 \begin{enumerate}
 \item Only WordNet triples (WN)
 \item Only GermaNet triples (GN)
 \item WordNet-GermaNet triples with one-direction cross-lingual alignments (WN-GN)
 \item WordNet-GermaNet with double-direction cross-lingual alignments (WN-GN DD)
 \end{enumerate}
 
 Dataset 3 includes both relations extracted from WordNet and GermaNet and also the mapping between senses.
 Dataset 4 is same as dataset 3 but since the models we will use are assuming all the relations are assymetric 
 we will try to encode the symmetry of cross-lingual alignments by reversing each of them and include the reverse in the dataset.
 Datasets 3 and 4 contains two different variants: the first variants contains only WordNet relations (test on WN) 
 in the held-out test dataset and the second variant contains only GermaNet triples (test on GN).
  In this way we can observe the direction of
  possibly transferring information from English to German or vice versa.
  
  For reducing the sparsity of data and boosting the learning runtime we 
  filtered out all the entities that appeared less than 3 times in our datasets.
   
  (version of wordnet, germanet, ILI and role of uby should be described here)  
 

\section{Evaluation}
\label{sec:eval}

To show the effectiveness of joint learning of features from multiple knowledge bases we suggest 
two experiment setups. In the first schema we follow Bordes et al. ranking task. The goal of this task is
to show how well information in knowledge bases can be preserved by learned features. 
 On the other hand, the second
setup is investigating on this question that if bi-lingual word embeddings from
multiple resources are able to improve the performance of mono-lingual embeddings in a standard NLP
task, here word-pair similarity or not.
In this setup we will look to contribution of the learned features in predicting similarity of words.

\subsection{Intrinstic Evaluation}
\label{ssec:intrinsic}

Bordes et al. (Bordes2011) proposed a ranking task that for each triple $(e_{i} , r_{k}, e_{j} )$ in the data set,
 all the entities will be ranked as a candidate for being right entity of the triple 
 given the relation and the left entity. Depends on which one of the models is used, SE or SME-Bil, all the entities will be sorted
  based on their score regarding Equation~\eqref{eq:sim} or Equation~\ref{eq:dot} previously introduced in section~\ref{ssec:bordes}. 
  By keeping the statistics of difference between the predicted rank of $e_{j}$ and its true rank and also repeat the same process
  for left entities, we will be able to report the mean and median predicted rank of entities per relation and in total. Bordes et al.
   proposed to schema for calculating the average rank, micro averaging which emphasis on more frequent relations by
    weighted averaging with frequency of relations as weights and macro averaging which consider all the relations equally
    , either frequent or infrequent ones. The third statistic that we report following their work, r@100, is the ratio of number of times that 
    an entity is correctly among top 100 entities ranked and predicted for a triple to the number of occurances of this entity in the dataset.
    We applied SE and SME-Bil models on our created datasets and the ranking performance on each of them is presented in Table ~\ref{tbl:rank-tbl}.
	
\begin{table*}[ht]
\caption{Intrinsic Evaluation (Ranking Score Performance)  }
\label{tbl:rank-tbl} % title name of the table
\centering % centering table
{\footnotesize
\begin{tabular}{l  c c c c c}
\hline\hline % inserting double-line
 Dataset &  \#relations & \#entities &  & Micro & Macro
\\ [0.5ex] 
\hline % inserts single-line

 & &   & lhs & 84.42 & 72.59 \\[-0.5ex]
  & &   & rhs & 84.04 & 72.38 \\[-0.5ex]
  & &   & mean & 1003.59 & 3739.85 \\[-0.5ex]
  & &   & median & 5.0 & 2213.37 \\[-0.5ex]
\raisebox{1.0ex}{GN SE} &  \raisebox{0.5ex}{16}& \raisebox{0.5ex}{64025}
&global& 84.23 & 72.49 \\[1ex]

 & &   & lhs & 79.06 & 58.58 \\[-0.5ex]
  & &   & rhs & 83.30 & 81.11 \\[-0.5ex]
  & &   & mean & 407.90 & 308.01 \\[-0.5ex]
  & &   & median & 10.0 & 54.18 \\[-0.5ex]
\raisebox{1.0ex}{GN SME-Bil} &  \raisebox{0.5ex}{16}& \raisebox{0.5ex}{64025}&global
&  81.18 & 69.85 \\[1ex]
\hline

 & &   & lhs & 91.90 & 89.47 \\[-0.5ex]
  & &   & rhs & 92.30 & 90.25 \\[-0.5ex]
  & &   & mean & 148.72 & 623.10 \\[-0.5ex]
  & &   & median & 5.0 & 4.69 \\[-0.5ex]
\raisebox{1.0ex}{WN SE} &  \raisebox{0.5ex}{23}& \raisebox{0.5ex}{148976}& global
& 92.10 & 89.86 \\[1ex]

& &   & lhs & 83.08 & 72.21 \\[-1ex]
  & &   & rhs & 85.2 &77.92 \\[-1ex]
  & &   & mean & 128.82 & 511.21 \\[-0.5ex]
  & &   & median & 10.0 & 26.63 \\[-0.5ex]
\raisebox{1.0ex}{WN SME-Bil} &  \raisebox{0.5ex}{23}& \raisebox{0.5ex}{148976}& global
& 84.14 & 75.57 \\[1ex]
\hline

 & &   & lhs & 90.82 & 89.14  \\[-0.5ex]
  & &   & rhs & 91.56 & 88.76 \\[-0.5ex]
  & &   & mean & 293.16 & 1356.30 \\[-0.5ex]
  & &   & median & 5.0 & 5.10 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SE (WN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 91.19 & 88.95 \\[1ex]

 & &   & lhs & 82.42 & 73.65 \\[-0.5ex]
  & &   & rhs & 83.40 & 73.44 \\[-0.5ex]
  & &   & mean & 124.85 & 331.82 \\[-0.5ex]
  & &   & median & 11.0 & 33.86 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SME-Bil(WN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 82.91 & 73.55 \\[1ex]

 \hline
 & &   & lhs & 81.82 & 70.56  \\[-0.5ex]
  & &   & rhs & 79.92 & 70.06 \\[-0.5ex]
  & &   & mean & 3031.44 & 15470.56 \\[-0.5ex]
  & &   & median & 7.0 & 10080.5 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SE (GN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 80.87 & 70.313 \\[1ex]

 & &   & lhs & 63.54 & 41.64 \\[-0.5ex]
  & &   & rhs & 64.78 & 70.32 \\[-0.5ex]
  & &   & mean & 984.79 & 1021.37 \\[-0.5ex]
  & &   & median & 40.0 & 428.90 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SME-Bil(GN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 64.16 & 55.98 \\[1ex]
 \hline
 
& &   & lhs & 77.06 & 64.98 \\[-0.5ex]
  &  &  & rhs & 77.08 & 65.17 \\[-0.5ex]
  & &   & mean & 166.18 & 466.91 \\[-0.5ex]
  & &   & median & 18.0 & 55.41 \\[-0.5ex]
\raisebox{1.0ex}{WordNet-GermaNet-DD (WN held out)} &  \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 77.07 & 65.082 \\[1ex]
 
& &   & lhs & 57.72 & 38.06 \\[-0.5ex]
  &  &  & rhs & 60.72 & 63.61 \\[-0.5ex]
  & &   & mean & 932.49 & 719.47 \\[-0.5ex]
  & &   & median & 56.0 & 175.56 \\[-0.5ex]
\raisebox{1.0ex}{WordNet-GermaNet-DD (GN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 59.22 & 50.84 \\[1ex]
 
% [1ex] adds vertical space
\hline % inserts single-line
\end{tabular}
}
\label{tab:PPer}
\end{table*}
   

\iffalse 
\begin{table*}[ht]
\caption{Ranking Performance for Mapped Relations } % title name of the table
\centering % centering table
\begin{tabular}{l c c c c c c} % creating 10 columns
\hline\hline % inserting double-line
 Dataset & \#dimension & \#relations & \#entities &  & Micro(\%) & Macro(\%)
\\ [0.5ex] 
\hline % inserts single-line

 & & &  & lhs & 82.60 & 68.18 \\[-1ex]
  & & &  & rhs & 81.90 & 68.84 \\[-1ex]
\raisebox{1.5ex}{GermaNet} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{10}& \raisebox{0.5ex}{64025}&global
&  82.25 & 68.51 \\[1ex]

 & & &  & lhs & 83.50 & 83.17 \\[-1ex]
  & & &  & rhs & 84.22 & 83.64 \\[-1ex]
\raisebox{1.5ex}{WordNet} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{19}& \raisebox{0.5ex}{148976}& global
& 83.86 & 83.40 \\[1ex]

 & & &  & lhs & 78.70 & 82.60 \\[-1ex]
  & & &  & rhs & 79.56 & 83.06 \\[-1ex]
\raisebox{1.5ex}{WordNet-GermaNet (WN)} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{24}& \raisebox{0.5ex}{213002}& global
& 79.13 & 82.83 \\[1ex]

 & & &  & lhs & 69.66 & 59.54 \\[-1ex]
  & & &  & rhs & 66.60 & 58.95 \\[-1ex]
\raisebox{1.5ex}{WordNet-GermaNet (GN)} & \raisebox{0.5ex}{25}& \raisebox{0.5ex}{24}& \raisebox{0.5ex}{213002}& global
& 68.13 & 59.25 \\[1ex]


% [1ex] adds vertical space
\hline % inserts single-line
\end{tabular}
\label{tab:PPer}
\end{table*}
\fi
    

\subsection{Extrinsic Evaluation}
\label{ssec:extrinsic}
 \label{exp:word-similarity}
 We are interested to further analyze the effectiveness of learned embeddings to capture semantic features of words,
  therefore 
 we compare the mono-lingual and bi-lingual embeddings against human judgments
 and also other embeddings learned from corpus. 
 The other embeddings which we used for our comparison are (Turian et al., HLBL
 and Klementiev et al.).
 To measure the similarity between any given wordpair $(w_1 , w_2)$ we find all vectors associated to different senses
 of the given words in our embedding dictionary and pick the pair of embeddings that maximize cosine
  similarity between two words. 
 We can motivate this by saying that for each word pair any of words works as a context for 
 disambiguating the sense of the other word.
 
 Four datasets of word-pair similarity are used to compare correlation of
 predicted similairty of pair of words against human judgments.
 RG-65 [rubensteinGoodenough], Yang and Powers verb similarity
 dataset[yangPowers], MC-30[millerCharles] and WS-353 [finkelstein] are English
 datasets that we used for this task. RG-65 and its subset, MC-30 are providing
 human scored datasets for measuring synonymy among wordpairs (nouns),. WS-353
 has broader notaion of semantic similarity and include word pairs for measuirng semantic
 relatedness too. Yang and Powers have provided a dataset for measuring semantic
 similarity between verbs.
  
 
  
 Both Pearson and Spearman correlation of predicted and gold similarities 
 are calculated and is reported in table \ref{tbl:en-wp-sim} for English. We can
 see that bi-lingual embeddings learned by SME-Bil model outperformed all other
 embeddings both in Pearson and Spearman correlation in all four datasets.
 Another observation is that SME-Bil models performes better than SE
 models in most cases. Among SME-Bil models, bi-lingual embeddings are always
 more correlated to human hudgments than mono-lingual embeddings.
 All models perform poorly on WS-353, bi-lingual SME-Bil model still performs
 better than the rest. We believe this could due to including notion of
 relatedness in this dataset. Poor performance of knowledge based models on
 semantic relatedness task is previously known and discussed in
 [Szulmanski2013]. 
 

 
 \begin{table*}[ht]
\caption{Word-pair Similarity Performance for English } % title name of the table
\label{tbl:en-wp-sim}
\centering % centering table
\tabcolsep=0.09cm
{\footnotesize
\begin{tabular}{c c c c c c c c c c} 
\hline\hline % inserting double-line
 Dataset & & WN-SE  & WN-GN-SE & WN-SME-Bil &  WN-GN-SME-Bil & WN-GN-SME-Bil-DD &HLBL & Turian* & Klementiev*
\\ [0.5ex] 
\hline % inserts single-line
                          &P&0.682&0.666&0.703&0.833&0.725&-0.115&0.233&-0.380 \\[-1ex]
\raisebox{1.5ex}{RG-65}  &S&0.769&0.741&0.741&0.811&0.825&-.083&0.118&-0.398 \\[1ex]

                          &  P &0.611&0.644&0.601&0.740&0.599&-0.363&0.150&-0.768 \\[-1ex]
\raisebox{1.5ex}{MC-30}  &  S & 0.720&0.648&0.756&0.846&0.954&-.450&-0.198&-0.522 \\[1ex]

                           &  P &0.181&0.206&0.239&0.246&0.238&0.233&0.236&0.029 \\[-1ex]
\raisebox{1.5ex}{WS-353}  &  S &0.093&0.146&0.185&0.224&0.201&0.197&0.210&0.040 \\[1ex]

 %                           &  P
                            % &-0.109&0.037&-0.126&-0.326&-0.1128&-0.367&-0.044&-0.005 \\[-1ex]
%\raisebox{1.5ex}{Rel-122}  &  S &
% -0.112&0.019&-0.121&0.037&0.041&-0.409&-0.092&-0.0009 \\[1ex]

                                  &  P & 0.482&0.637&0.584&0.627&0.610&-0.130&-0.076&0.154 \\[-1ex]
\raisebox{1.5ex}{YangPowers-130}  &  S & 0.401&0.472&0.406&0.553&0.533&-0.186&-0.116& 0.113 \\[1ex]

\hline % inserts single-line
     
          
 \hline % inserts single-line
\end{tabular}
}
\label{tab:en-wp-sim}
\end{table*}      


For German,
 we use [this and that].

\begin{table*}[ht]
\caption{Word-pair Similarity Performance for German } % title name of the table
\label{tbl:de-wp-sim}
\centering  % centering table
\tabcolsep=0.09cm
{\footnotesize
\begin{tabular}{cr c c c c c c c} % creating 10 columns
\hline\hline % inserting double-line
 Dataset & & GN-SE  & WN-GN-SE & GN-SME-Bil &  WN-GN-SME-Bil & WN-GN-SME-Bil-DD & Klementiev*
\\ [0.5ex] 
\hline % inserts single-line
                                 &  P & -0.010  & 0.156 & 0.073 & 0.130&0.196 &0.107 \\[-1ex]
\raisebox{1.5ex}{wortpaare222}  &  S & -0.125 & 0.234& 0.152 & 0.175 & 0.111 &0.152 \\[1ex]

                                  &  P & 0.865 & 0.984 & 0.185 & 0.287 & 0.301 & -0.887 \\[-1ex]
\raisebox{1.5ex}{wortpaare30}    &  S & 1.0   & 1.0   & -0.500 & -0.500  & 0.500 & -1.0 \\[1ex]

                                  &  P & -0.085  & 0.063 & 0.185 & 0.188 & 0.127 &0.187 \\[-1ex]
\raisebox{1.5ex}{wortpaare350}  &  S & -0.157 & 0.009  &  0.172 & 0.194 & 0.142 &0.142 \\[1ex]

                                &  P & 0.800  & 0.558 & -0.572 & 0.485 & -0.166 & 0.233 \\[-1ex]
\raisebox{1.5ex}{wortpaare65}  &  S & 0.800 & 0.800 & -0.8 & 0.399 & 0.200 & 0.200 \\[1ex]


\hline % inserts single-line
     
          
 \hline % inserts single-line
\end{tabular}
}

\end{table*}      
          
            
     


 
\section{Conclusion and Future Work}



% If you use BibTeX with a bib file named eacl2014.bib, 
% you should add the following two lines:
\bibliographystyle{acl}
\bibliography{eacl2014}

% Otherwise you can include your references as follows:
%% \begin{thebibliography}{}

%% \bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
%% Alfred~V. Aho and Jeffrey~D. Ullman.
%% \newblock 1972.
%% \newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
%% \newblock Prentice-{Hall}, Englewood Cliffs, NJ.

%% \bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
%% {American Psychological Association}.
%% \newblock 1983.
%% \newblock {\em Publications Manual}.
%% \newblock American Psychological Association, Washington, DC.

%% \bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
%% {Association for Computing Machinery}.
%% \newblock 1983.
%% \newblock {\em Computing Reviews}, 24(11):503--512.

%% \bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
%% Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
%% \newblock 1981.
%% \newblock Alternation.
%% \newblock {\em Journal of the Association for Computing Machinery},
%%   28(1):114--133.

%% \bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
%% Dan Gusfield.
%% \newblock 1997.
%% \newblock {\em Algorithms on Strings, Trees and Sequences}.
%% \newblock Cambridge University Press, Cambridge, UK.

%% \end{thebibliography}

\end{document}
