\chapter{Representation Learning for Natural Language Processing}
\label{ch:word-embeddings}

\section{Learning Word Features from Corpus}
\label{ssec:lang-model}

In \cite{Collobert2008a}, Weston and Collobert use a non-probabilistic and
discriminative model to jointly learn word embeddings and a language model that
can separate plausible n-grams from noisy ones. For each word in a n-gram, they
combine the word embeddings and use it as positive example. They put noise in
the n-gram to make negative examples and then train a neural network to learn to
classify postive labels from negative ones. The parameters of neural network
(neural language model) and word embedding values will be learned jointly by an
optimization method called \emph{Stochastic Gradient Descent} \cite{Bottou2010}.

A hierarchical dsitributed language model (HLBL) proposed by Mnih and
Hinton in \cite{Mnih2009} is another influential work on word embeddings. In
this model a probabilistic linear neural network(LBL) will be trained to 
combine word embeddings in first $n-1$ words of a n-gram to predict the $n_th$ word.

Weston-Collobert model and HLBL by Mnih and Hinton are evaluated in
\cite{Turian2010b} in two NLP tasks: chunking and named entity recognition. With
using word embeddings from these models combined with hand-crafted features, the
performance of both tasks are shown to be improved.

\section{Learning Word Features from Lexical Resources}
\label{sec:structured-word-embedding}

Bordes et al. in \cite{Bordes2011} and \cite{Bordes2012} have attempted to use
a neural distributed model to induce word representations from lexical resources
such as WordNet and knowledge bases (KB) like Freebase. In Freebase for example, each named entity is related
to another entity by an instance of a specific type of relation. In
\cite{Bordes2011}, each entity is represented as a vector and each relation is decomposed to two
matrices. Each of these matrices transform left and right-hand-side entities
to a semantic space. Similarity of transformed entities indicates that the
relation holds between the entities.  A prediction task is defined to evaluate
the embeddings. Given a relation and one of the entities, the task is to predict
the missing entity. The high accuracy (99.2\%) of the model on prediciton
of training data shows that learned representation highly captures attributes of
the entities and relations in Freebase.

Two major models are proposed in [Bordes2011] and [Bordes2012] to learn features in continues vector space
 from a Knowledge Bases(KB) which information is usually representated in form of triples of $(e_{i},r_{k} , e_{j} )$ 
 where $e_{i}$ and $e_{j}$ are $i_{th}$ and $j_{th}$ entities related
 by a binary relation of type $r_{k}$. The purpose of the models is to induce a vector space and associate
  each entity or relation to an embedding vector or a matrix.
  The dimensions of such an embedding vector are supposed to reflect a set of informative features of entities and relations.
   
   In the first model, \textbf{structured embeddings(SE)}, entities are modeled as \textit{d}-dimensional vectors.
    An associated vector to the $i_{th}$ entity, $e_{i}$, is $E_{i} \in \mathbb{R}^{d}$. Each relation $r_{k}$  
    is decomposed to two operators each represented as $d \times d$ matrix, $ R_k = (R_{k}^{left}, R_{k}^{right})$. 
    These operators transform the left and right entities to a new space induced by each relation and by using 
    a $p$-norm measure  (L1 norm in this work) they associate a similarity value or a score to each triple. 
    This similarity value is being calculated by
    Equation ~\eqref{eq:sim}. 
    \begin{equation}
    \label{eq:sim}Sim(E_{i}, E_{j}, R) = ||R_{k}^{left}E_{i} - R_{k}^{right}E_{j} ||_{1}
    \end{equation}
    The similarity between transformed entities works as a score to measure the strength of a relation holds between two entities. 
      
    Using the idea of contrastive learning , the model will be trained to increase similarity of 
    embeddings for a positive triple (a triple which exists in the KB) or lowering its rank among other training samples
    and decrease the similarity of embeddings when the relation doesn't hold (negative triple) or raising its rank . 
    For each positive triple, two negative triples will be generated by randomly alternating the right entity or left entity with other entities.
    Inspired from large margin methods a constraint is introdcued on the model that forces 
    negative triples to have lower associated similarity value  than correspondent positive triples by a large margin.
    
    The second model, \textbf{Semantic Matching Energy using Bilinear layers(SME-Bil)}, 
    is using a different represention for relations,weighted bilinear transformation of embeddings 
    and  dot product similarity function instead of L1 norm. 
    In this model, each relation is represented by a \textit{d}-dimensional vector $R_{k}$ same as entities. 
    For triple $(e_{i},r_{k} , e_{j} )$ , the model combines the weighted transformation of each entity embedding with 
    the weighted embedding of relation using element-wise vector product. as it is shown in Equation ~\eqref{eq:bil}.
    \begin{equation}
    \label{eq:bil} E'_{left} = (W_{i} E_{i}) \odot (W_{k} R_{k}) + b_{left}
    \end{equation}
    $W_{i}$ and $W_{k}$ are $d \times d$ weight matrices and $b_{left}$ is a $d$-dimensional bias vector. 
    The same equation holds for transforming the right entity embeddings to $E'_{right}$. Finally, the associated score for the triple
     can be calucated by dot product of $E'_{left}$ and $E'_{right}$ which is shown in Equation ~\ref{eq:dot}.
    
   \begin{equation}
    \label{eq:dot} Sim(E_{i}, E_{j}, R_{k}) = -E'_{left}E'_{right}
   \end{equation}
    Similar constraints to the first model are also applied to this model and 
     both models can be trained by stochahstic gradient descent (SGD) which we
     introduced in \autoref{sec:SGD}.


