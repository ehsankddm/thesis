\chapter[Entity Linking Among Lexical R]{Entity Linking Among Lexical Resources\footnote{This part of the work has been done during an internship in UKP
Lab, Technical University of Darmstadt. All the codes and results produced during
this time belongs to Prof. Dr. Iryna Gurevych and can be obtained per
request.}}

\label{ch:ent-link}


Each KB or lexical resource contains limited amount of information and
different structured resources have different perspective and partially encode
information about an entity and its relations to the other entities of the
resource. Therefor, if one is interested about learning different features from
a lexical resource or a KB, a natural next step is to jointly learn features
from multiple resources. In previous chapter we saw how to learn features from
a KB and a corpus with application to relation discovery and extraction. In this
chapter we focus on inducing features for entities of multiple structured KBs.
For relation extraction, the most relevant available resource is Freebase and
it suffices together with corpus to learn features of named entities (NE). In
this chapter we focus on learning features for other type of words e.g. nouns,
verbs and \ldots~. Learning features for other words in a sentence rather than
just NEs might help improving the performance of a relation discovery system
specially for verbs (relations) when some semantic aspects of them like
selectional preference has been shown to be an important aspect. Another
application is learning and linking entities of lexical resources in two
different languages which helps to transfer information from one language domain
to the another one. For these reasons, first we describe a pipeline to learn
word features from multiple resources and as proof of concept I implement this
idea with inducing bi-lingual word features and show its performance
on semantic similarity task. The actual application of this idea and learned
word features is postponed to future work which I describe potential usage of
them in relation discovery.


\section{Task Description}
\label{sec:xkb-task-desc}

In this part I describe the methodology we followed to encode available information 
in two different lexical resources, WordNet and GermaNet ~\cite{BirgitHamp},
that makes it possible to link entities of the two different resources and learn bi-lingual 
embeddings of word senses in German and English.  
The main idea is to relate two senses from two different resources using cross-lingual sense alignments.
This is an additional information which can play a role of bridge between two
different tasks, learning German embeddings and English embeddings, and can help to transfer knowledge from one to the another.
Using this new feature we make our WordNet-GermaNet dataset which contains three
types of relations
(1) WordNet relations 
(2) GermaNet relations
(3) Cross-lingual sense alignments between WordNet and GermaNet

First two types of relations are directly extracted from WordNet and GermaNet and for the 
cross-lingual relations we used Interlingual Index mappings between WordNet and GermaNet.
\\
Example of relations:
\begin{center}
WN-sense-A \hspace{0.3in}  WN-rel-1    \hspace{0.3in}  WN-sense-B\\
GN-sense-C \hspace{0.3in}  GN-rel-2    \hspace{0.3in}  GN-sense-D\\
WN-sense-A \hspace{0.3in}  ILI-rel-1-2 \hspace{0.3in}  GN-sense-D\\
\end{center}

Left and right entities are WordNet and GermaNet senses and relations are current semantical relations
 in each of lexicons such as:
 meronymy, holonymy and \ldots.


\section{Experimental Setup}
\label{sec:xkb-exp}

 
 We have created four different dataset, each divided to train, test and validation separated subsets. Our four datasets are:
 \begin{enumerate}
 \item Only WordNet triples (WN)
 \item Only GermaNet triples (GN)
 \item WordNet-GermaNet triples with one-direction cross-lingual alignments (WN-GN)
 \item WordNet-GermaNet with double-direction cross-lingual alignments (WN-GN DD)
 \end{enumerate}
 
 Dataset 3 includes both relations extracted from WordNet and GermaNet and also the mapping between senses.
 Dataset 4 is same as dataset 3 but since the models we will use are assuming all the relations are assymetric 
 we will try to encode the symmetry of cross-lingual alignments by reversing each of them and include the reverse in the dataset.
 Datasets 3 and 4 contains two different variants: the first variants contains only WordNet relations (test on WN) 
 in the held-out test dataset and the second variant contains only GermaNet triples (test on GN).
  In this way we can observe the direction of
  possibly transferring information from English to German or vice versa.
  
  For reducing the sparsity of data and boosting the learning runtime we 
  filtered out all the entities that appeared less than 3 times in our datasets.
   
  For implementation I used a unified lexical resource called \textit{Uby}
  ~\cite{Eckle-Kohler2012} which provides an integrated API to work with
  Wordnet, GermaNet and other resources.
 

\section{Evaluation}
\label{sec:ent-link-eval}

To show the effectiveness of joint learning of features from multiple knowledge bases we suggest 
two experiment setups. In the first schema we follow Bordes et al. ranking task. The goal of this task is
to show how well the information in knowledge bases can be preserved by the learned features. 
 On the other hand, the second
setup is investigating on this question that if the learned word embeddings from multiple resources
are able to improve the performance of monolingual embeddings in a standard NLP
task, here word-pair similarity or not.
In this setup we will look to contribution of the learned features in predicting similarity of words.

\subsection{Evaluation Using Reconstruction}
\label{ssec:ent-link-intrinsic}

Bordes et al. (Bordes2011) proposed a ranking task that for each triple $(e_{i} , r_{k}, e_{j} )$ in the data set,
 all the entities will be ranked as a candidate for being right entity of the triple 
 given the relation and the left entity. Depends on which one of the models is used, SE or SME-Bil, all the entities will be sorted
  based on their score regarding Equation~\eqref{eq:sim} or Equation~\ref{eq:dot} previously introduced in section~\ref{ssec:bordes}. 
  By keeping the statistics of difference between the predicted rank of $e_{j}$ and its true rank and also repeat the same process
  for left entities, we will be able to report the mean and median predicted rank of entities per relation and in total. Bordes et al.
   proposed to schema for calculating the average rank, micro averaging which emphasis on more frequent relations by
    weighted averaging with frequency of relations as weights and macro averaging which consider all the relations equally
    , either frequent or infrequent ones. The third statistic that we report following their work, r@100, is the ratio of number of times that 
    an entity is correctly among top 100 entities ranked and predicted for a triple to the number of occurrences of this entity in the dataset.
    We applied SE and SME-Bil models on our created datasets and the ranking performance on each of them is presented in Table ~\ref{tbl:rank-tbl}.
	
\begin{table}[ht]
\caption{Intrinsic Evaluation (Ranking Score Performance)  }
\label{tbl:rank-tbl} % title name of the table
\centering % centering table

\begin{tabular}{l  c c c c c}
\hline\hline % inserting double-line
 Dataset &  \#relations & \#entities &  & Micro & Macro
\\ [0.5ex] 
\hline % inserts single-line

  & &   & mean & 1003.59 & 3739.85 \\[-0.5ex]
  & &   & median & 5.0 & 2213.37 \\[-0.5ex]
\raisebox{1.0ex}{GN SE} &  \raisebox{0.5ex}{16}& \raisebox{0.5ex}{64025}
&global& 84.23 & 72.49 \\[1ex]

  & &   & mean & 407.90 & 308.01 \\[-0.5ex]
  & &   & median & 10.0 & 54.18 \\[-0.5ex]
\raisebox{1.0ex}{GN SME-Bil} &  \raisebox{0.5ex}{16}& \raisebox{0.5ex}{64025}&global
&  81.18 & 69.85 \\[1ex]
\hline

  & &   & mean & 148.72 & 623.10 \\[-0.5ex]
  & &   & median & 5.0 & 4.69 \\[-0.5ex]
\raisebox{1.0ex}{WN SE} &  \raisebox{0.5ex}{23}& \raisebox{0.5ex}{148976}& global
& 92.10 & 89.86 \\[1ex]

  & &   & mean & 128.82 & 511.21 \\[-0.5ex]
  & &   & median & 10.0 & 26.63 \\[-0.5ex]
\raisebox{1.0ex}{WN SME-Bil} &  \raisebox{0.5ex}{23}& \raisebox{0.5ex}{148976}& global
& 84.14 & 75.57 \\[1ex]
\hline

  & &   & mean & 293.16 & 1356.30 \\[-0.5ex]
  & &   & median & 5.0 & 5.10 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SE (WN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 91.19 & 88.95 \\[1ex]

  & &   & mean & 124.85 & 331.82 \\[-0.5ex]
  & &   & median & 11.0 & 33.86 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SME-Bil(WN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 82.91 & 73.55 \\[1ex]

 \hline
  & &   & mean & 3031.44 & 15470.56 \\[-0.5ex]
  & &   & median & 7.0 & 10080.5 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SE (GN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 80.87 & 70.313 \\[1ex]

  & &   & mean & 984.79 & 1021.37 \\[-0.5ex]
  & &   & median & 40.0 & 428.90 \\[-0.5ex]
\raisebox{1.0ex}{WN-GN SME-Bil(GN held out)} & \raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global
& 64.16 & 55.98 \\[1ex]
 \hline
 
  & &   & mean & 166.18 & 466.91 \\[-0.5ex]
  & &   & median & 18.0 & 55.41 \\[-0.5ex]
\raisebox{1.0ex}{WordNet-GermaNet-DD SME-Bil (WN held out)} & 
\raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global & 77.07 & 65.082 \\[1ex]
 
  & &   & mean & 932.49 & 719.47 \\[-0.5ex]
  & &   & median & 56.0 & 175.56 \\[-0.5ex]
\raisebox{1.0ex}{WordNet-GermaNet-DD SME-Bil (GN held out)} &
\raisebox{0.5ex}{32}& \raisebox{0.5ex}{213002}& global & 59.22 & 50.84 \\[1ex]

\hline % inserts single-line
\end{tabular}
\label{tab:PPer}
\end{table}
   


\subsection{Evaluation on Semantic Similarity}
\label{ssec:ent-link-extrinsic}
 \label{exp:word-similarity}
 We are interested to further analyze the effectiveness of learned embeddings to capture semantic features of words,
  therefore 
 we compare the mono-lingual and bi-lingual embeddings against human judgments
 and also other embeddings learned from corpus. 
 The other embeddings which we used for our comparison are Turian et al.
 \cite{Turian2010b}, Mnih et al. \cite{Mnih2009} mentioned in ~\autoref{sec:disted-repr} and
 Klementiev et al.
 \cite{Klementiev} embeddings To measure the similarity between any given word
 pair $(w_1 , w_2)$ we find all vectors associated to different senses of the given words in our embedding dictionary 
 and pick the pair of embeddings that maximize cosine
  similarity between two words. 
 We can motivate this by saying that for each word pair any of words works as a context for 
 disambiguating the sense of the other word.
 
 Four datasets of word-pair similarity are used to compare correlation of
 predicted similairty of pair of words against human judgments.
 RG-65 ~\cite{Rubenstein1965}, Yang and Powers verb similarity
 dataset~\cite{Yang2007}, MC-30~\cite{Miller1991} and WS-353
 ~\cite{Finkelstein2001}
  are English datasets that we used for this task. RG-65 and its subset, MC-30 are providing
 human scored datasets for measuring synonymy among wordpairs (nouns),. WS-353
 has broader notion of semantic similarity and include word pairs for measuring
 semantic relatedness too. Yang and Powers have provided a dataset for measuring semantic
 similarity between verbs.
  
 
  
 Both Pearson and Spearman correlation of predicted and gold similarities 
 are calculated and is reported in table \ref{tbl:en-wp-sim} for English. We can
 see that bi-lingual embeddings learned by SME-Bil model outperformed all other
 embeddings both in Pearson and Spearman correlation in all four datasets.
 Another observation is that SME-Bil models performs better than SE
 models in most cases. Among SME-Bil models, bi-lingual embeddings are always
 more correlated to human judgments than mono-lingual embeddings.
 All models perform poorly on WS-353, bi-lingual SME-Bil model still performs
 better than the rest. We believe this could due to including notion of
 relatedness in this dataset. Poor performance of WordNet based models on
 semantic relatedness task is previously known and discussed in
 ~\cite{Szumlanski}. Words can be related by different type of relations
 while in WordNet, a few number of relations are encoded e.g. synonymy which is the
 subject of our evaluation task.
 

 
 \begin{table}[ht]
\caption{Word-pair Similarity Performance for English } % title name of the table
\label{tbl:en-wp-sim}
\centering % centering table
\tabcolsep=0.09cm
{\footnotesize
\begin{tabular}{c c c c c c c c c c} 
\hline\hline % inserting double-line
 Dataset & & WN-SE  & WN-GN-SE & WN-SME-Bil &  WN-GN-SME-Bil & WN-GN-SME-Bil-DD
 &HLBL& Turian et al.& Klementiev et al
\\ [0.5ex] 
\hline % inserts single-line
                          &P&0.682&0.666&0.703&0.833&0.725&-0.115&0.233&-0.380 \\[-1ex]
\raisebox{1.5ex}{RG-65}  &S&0.769&0.741&0.741&0.811&0.825&-.083&0.118&-0.398 \\[1ex]

                          &  P &0.611&0.644&0.601&0.740&0.599&-0.363&0.150&-0.768 \\[-1ex]
\raisebox{1.5ex}{MC-30}  &  S & 0.720&0.648&0.756&0.846&0.954&-.450&-0.198&-0.522 \\[1ex]

                           &  P &0.181&0.206&0.239&0.246&0.238&0.233&0.236&0.029 \\[-1ex]
\raisebox{1.5ex}{WS-353}  &  S &0.093&0.146&0.185&0.224&0.201&0.197&0.210&0.040 \\[1ex]

 %                           &  P
                            % &-0.109&0.037&-0.126&-0.326&-0.1128&-0.367&-0.044&-0.005 \\[-1ex]
%\raisebox{1.5ex}{Rel-122}  &  S &
% -0.112&0.019&-0.121&0.037&0.041&-0.409&-0.092&-0.0009 \\[1ex]

                                  &  P & 0.482&0.637&0.584&0.627&0.610&-0.130&-0.076&0.154 \\[-1ex]
\raisebox{1.5ex}{YangPowers-130}  &  S & 0.401&0.472&0.406&0.553&0.533&-0.186&-0.116& 0.113 \\[1ex]

\hline % inserts single-line
     
          
 \hline % inserts single-line
\end{tabular}
}
\label{tab:en-wp-sim}
\end{table}      


For German,
 we use translated versions of RG-65, MC-30 and WS-353 which were judged by
 German native speakers ~\cite{Zesch2008}, additionally a dataset from
 ~\cite{Zesch2006} is used. The structured embeddings are being
 compared to the only available German word embeddings from
 \cite{Klementiev}. We can observe that bi-lingual embeddings
 always outperform mono-lingual embeddings except for Gur65 dataset which
 both mono-lingual and bi-lingual \textbf{SE} models have equal Spearman
 correlation to the gold data but mono-lingual model performs better in Pearson
 correlation. The good results of German evaluation is another indicator for
 proofing the idea of learning word embeddings from multiple resources.

\begin{table*}[ht]
\caption{Word-pair Similarity Performance for German } % title name of the table
\label{tbl:de-wp-sim}
\centering  % centering table
\tabcolsep=0.09cm
{\footnotesize
\begin{tabular}{cr c c c c c c c} % creating 10 columns
\hline\hline % inserting double-line
 Dataset & & GN-SE  & WN-GN-SE & GN-SME-Bil &  WN-GN-SME-Bil & WN-GN-SME-Bil-DD
 & Klementiev et al.
\\ [0.5ex] 
\hline % inserts single-line
                                 
                                 &  P & -0.010  & 0.156 & 0.073 & 0.130&0.196 &0.107 \\[-1ex]
\raisebox{1.5ex}{ZG222}  &  S & -0.125 & 0.234& 0.152 & 0.175 & 0.111 &0.152 \\[1ex]

                                  &  P & 0.865 & 0.984 & 0.185 & 0.287 & 0.301 & -0.887 \\[-1ex]
\raisebox{1.5ex}{Gur30}    &  S & 1.0   & 1.0   & -0.500 & -0.500  & 0.500 &
-1.0
\\[1ex]

                                  &  P & -0.085  & 0.063 & 0.185 & 0.188 & 0.127 &0.187 \\[-1ex]
\raisebox{1.5ex}{Gur350}  &  S & -0.157 & 0.009  &  0.172 & 0.194 & 0.142
&0.142
\\[1ex]

                                &  P & 0.800  & 0.558 & -0.572 & 0.485 & -0.166 & 0.233 \\[-1ex]
\raisebox{1.5ex}{Gur65}  &  S & 0.800 & 0.800 & -0.8 & 0.399 & 0.200 & 0.200
\\[1ex]
                                 


\hline % inserts single-line
     
          
 \hline % inserts single-line
\end{tabular}
}

\end{table*}      

\section{Conclusion}
\label{sec:ent-link-conc}
In this chapter the idea of learning word embeddings was motivated by first
formulating the task of learning bi-lingual word embeddings from WordNet and
GermaNet and then evaluating the embeddings on semantic similarity task.
Variation of models used to induce and compare word embeddings and the better
performance of bi-lingual word embeddings in compare to mono-lingual and the
other models to gold human judgments for word pair semantic similarity datasets
showed the effectiveness of this idea. These induced word features, specially
for verbs, have the potential of being used in a relation discovery system as an
extra information. By including more resources, which is trivial in the proposed
framework different aspects of a word or named enitiy can be learned which can
even increase the performance of current systems potentially. 
          