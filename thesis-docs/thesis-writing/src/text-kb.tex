\chapter{Linking Text to a Knowledge Base for Relation Discovery}
\label{ch:text-kb}

In this chapter I introduce an idea which relates previous work on
representation learning of KBs to relation extraction from text. First I propose
a method which tries to learn embeddings of entities and relations both from
Freebase and a corpus prepared in a specific format. With such a rich
embeddings, I will propose a model to and settings to predict links between
entities in Freebase or relations among entities mentioned in the corpus.
Later on, the experimental setup and a pipeline created for this purpose will be
described and finally in the last section we will evaluate our model and discuss
different aspects of the results.

\section{Learning Representaion of Entities and Relations
from Text and Knowledge Base}
\label{sec:dataset}

Two direction of works have been conducted previously on learning representation
of words which we discussed in \ref{repr-learn}. 1) KB-based
representation learning and 2)Corpus-based methods. Here I will demonstrate a
process which will enable us to jointly learn embeddings with both contextual
and lexical information. Among previously discussed methods, I borrow a model
proposed in \cite{Bordes2012}. As I described in \ref{bordes-model}, this model
can take set of binary predicates and their arguments and induce continuous
features for both predicates and arguments in vector space. Using these
embeddings and a bilinear combination of them, the model can discriminate
between true facts in the dataset and negative examples. For more information of
this model please see \ref{bordes-model}.

We can see that this models in its plain vanilla form is limited to learn
embeddings from a KB but has a great potential to be extended in various
aspects. One of the possible aspects which matters for relation extraction task,
which is main motivation of this work, is that it should be able to learn facts
also from text. The reason is that, despite the vast effort of gathering
information and encode it as knowledge in KBs, they are limited in the sense of
coverage. To compensate this problem we need to discover new facts from corpus.

In chapter \ref{rel-dscv} we have extensively discussed major related works and
we also discussed important features that were being used in many important
works and are shown to be very effective. Most of these feature were previously
encoded in a way that can be used for classic classifiers. I will first
enumerate these features and then we will see that how can we formalized them
to make it possible to use them with a neural distributed model discussed in
\ref{bordes-model}. A thorough discussion of creating dataset will appear in
\ref{sec:textkb-exp} but for now, it is sufficient to know that by a corpus we
mean, a pre-processed clean pile of text in English, coming from harvested web
pages or from news papers which contains these annotations: 1) part of speech
tags 2)dependency parsed 3) named entities are recognized and tagged by their
type. Our process of extracting features will be described fully in
\ref{sec:textkb-exp}, in the next part, Any feature I describe comes from a
sentence which contains two named entities. 

\subsection{Informative Features for Relation Extraction}
In this part, I briefly enumerate important features which have been used
extensively in previous works. For an actul example, I show some instances from
a dataset prepared by Riedel et al. and used in \cite{Riedel2013} and
\cite{Riedel2010a}. For full description of futures in their work please see
\ref{UnivSchema}.

A relevent set of features that I harvest from text to my work is as follow:

\begin{description}
  
  \item[Type of Named Entities] \hfill \\
    These types are simple set of types that usually named entity taggers tag
    entities with them. For example: \textbf{LOC} for locations, \textbf{PER}
    for persons. These types contain a minimalistic information but yet useful.
    The actual type of entities, features of an entity which are common with
    similar entities should be induced by the model.
    
  \item[Dependency Role of Named Entities] \hfill \\
    Dependency structures have useful information and there is a long history of
    using dependency patterns in literature. For example we discussed
    \cite{Dirt}, \cite{patty} in \ref{rel-dscv} and we showed the importance of
    dependency roles in these models. I follow previous works and will use this
    type of features in my proposed model. As an example a NE in a senctence can
    have either of these roles: direct object \textit{dobj}, passive object
    \textit{pobj}, appositional modifier \textit{appos}, participial modifier
    \textit{partmod}, nominal subject \textit{nsubj}, noun compound modifier
    \textit{nn} and prepositional modifier \textit{prep}.
 
  \item[Head of Sentence in Dependency Path] \hfill \\
  
  The dependency path between arguments and the head of the sentence worked as
  surface patterns indicating a relation if it appears in different contexts.
  All the head words are collected and I use them as surface patterns. They are
 labels for relations that we want to induce or predict among entities. 
  
  
 \end{description}
 
 Another set of features, the most important one, which are available in KBs are
 actual knowledge about entities and the relation between them. This types of
 relations naturally are encoded as predicate-arguments relations. For example
 this relation:
\begin{center}
\label{rel:mir}
\textsc{ /people/person/nationality(~Mir-Hossein Mousavi,~Iran)}
\hfill
\end{center}

indicates that \textit{Mir-Hossein Mousavi} has \textit{Iranian} nationality.

\section{Task Description}
\label{sec:textkb-task}

We have a set of facts, $\mathcal{F}$, coming from a KB which any instance of it
is in form of a triple $(e_{i}, rel_{k}, e_{j})$ which arguments of this triple are
left and right named entities and the predicate is a relation from a certain
type. Additionally we have a corpus of text, $\mathcal{C}$, which is tagged by
part of speech, named entities and parsed with a dependency parser.
A positive triple is a triple which at least an element of context in
$\mathcal{C}$, here a sentence, supports this triple. Likewise, a negative
triple is a triple that there is no support available for it in the corpus.
Given $\mathcal{F}$ and $\mathcal{C}$ we would like to perform two tasks
jointly:

\begin{enumerate}
  \item Learning embeddings of named entities, KB relations and surface patterns
  from information in $\mathcal{F}$ and $\mathcal{C}$.
  \item Learning a model with objective of ranking all positive triples lower
  than all negative triples, preferably with a large margin.
\end{enumerate}

In the domain of machine learning there are several models with ability of
performing learning representations and classification jointly which we
discussed some of them in \autoref{repr-learning}. Among those models relevant
to our task, we picked the neural distributed model proposed in \cite{Bordes2012}.
A detail of this model is discussed in \autoref{Bordes}.

This model takes a set of triples $\mathcal{T}$ as an input and perform both of
our desired tasks. With having this model the only problem now is to generate 
$\mathcal{T}$ such as it staisfies our objective. In the next section I will
describe a method for generating this dataset and later in
\autoref{sec:textkb-eval} I emperically show the effectivness of this method.

\section{Linking Text to KB}
\label{sec:textkb-link}

To benefit from information both available in a KB and hidden in the text we
need to link them so information can transfer from one to the another. This can
be happened through sharing the task of learning representations of words
specially NEs and surface patterns. This section is dedicated to elaborate on
the approach I took to share the task of representation learning between text
and KB.

The main idea is to present important features of text in such a formalization
that can be mutually learned with set of facts that come from a KB, for instance
Freebase. We mentioned that knowledge in Freebase is encoded as triples of a
predicate and two arguments. We take the same formalism and by introducing
auxillary predicates, we encode the annotations of a corpus in form of triples.

These are list of auxiliary relations which we add to $\mathcal{T}$ with their
definition:
\begin {description}
\item[HAS\_TYPE] \hfil \\this predicate takes a NE as left argument and its type
produced by NE-tagger as right argument. For example:
\begin{center}
 \textsc{HAS\_TYPE(~Mir-Hossein Mousavi,~Per)}
 \end{center}

\item[HAS\_DEP\_ROLE]\hfil \\ with this predicate we relate a NE to its
dependency role:

\begin{center}
 \textsc{ HAS\_DEP\_ROLE(~Mir-Hossein Mousavi,~dobj)}
 \end{center}
 
 \item[Head of Dependency path]\hfil \\ for each dependency path, we can
 consider head of a path as a relation or a predicate which relates two NEs of the path
 together. This head word ,which in our case is always verb but in general can
 also be a nominal phrase, will work as a surface pattern and a candidate
 relation. From now on we will call this head words \textit{triggers},
 following the work in \cite{UniversalSchema}.
 For example for this sentence and its dependency path:
\begin{center}
 \textit{Mir-Hossein Mousavi, president of Iran, said \ldots}
 \textsc{path\#appos|->appos->president->prep->of->pobj->|pobj }
 \end{center}
 
 we will have the relation below in $\mathcal{T}$:
 \begin{center}
 \textsc{PRESIDENT (~Mir-Hossein Mousavi,~Iran)}
 \end{center}
 


\item[HAS\_TRIGGER]\hfil \\we know that we have two types of relations in
$\mathcal{T}$, relations that come from corpus $\mathcal{C}$ and those from a KB
$\mathcal{F}$. If there are two NEs in $\mathcal{C}$ which are related by a
trigger and there exist a relation between them in $\mathcal{F}$ too then we
will add a third relation, \textit{HAS\_TRIGGER} between the knowledge base
relation and a trigger relation. For example given these two relations in
$\mathcal{T}$ :
\begin{center}
 \textsc{PRESIDENT (~Mir-Hossein Mousavi,~Iran)}
 \textsc{ /government/position\_held/president (~Mir-Hossein Mousavi,~Iran)}
 \end{center}
 
 we will add the third relation below to $\mathcal{T}$:
  \begin{center}
 \textsc{HAS\_TRIGGER (~/government/position_held/president,~PRESIDENT)}
 \end{center}
 
 The reason is to increase correlation between KB relations and text surface
 patterns which will enable the model to transfer information across KB and
 corpus.
 \end{description}
 
 Having $\mathcal{T}$ generated now we can run our experiments. In the next
 section I describe the pipeline  and after that we will see the evaluation of
 our approach.

\section{Experimental Setup}
\label{sec:textkb-exp}

In this section, first we will describe the process of producing a dataset I use
for running experiments and then we will see different experiments I ran to
examine the effectiveness of proposed approach for relation discovery.

\subsection{Creating Dataset}
\label{ssec:textkb-exp-data}

There are three phases to create a suitable dataset for our experiments. In the
first phase, a corpus should be annotated by part of speech and named entity
tags and parsed with dependency parser. This will lead to produce our desired
corpus $\mathcal{C}$ . In the second phase, we pick a KB and
use it as our $\mathcal{F}$ dataset. If $\mathcal{F}$ is too big in sense of
size of relations for our computing resource we can limit ourselves to a subset
of KB. I followed the heuristic proposed in \cite{Mintz2009} and
\cite{Riedel2013} and limit  $\mathcal{F}$ only to relations which their
argument have been co-occurred in a sentence in $\mathcal{C}$.

In the third phase, using $\mathcal{C}$ and $\mathcal{F}$ we produce triples
that we discussed in \autoref{sec:textkb-link} and create our final dataset
$\mathcal{T}$. 

To make our corpus we follow the protocol described in \cite{Riedel2013} and
\cite{Riedel2010} and use their dataset since it is fully compatible with our
requirements.
They used NYTimes corpusin their work. After pre-processing and tagging this
corpus, it is splitted to two parts. Articles after year 2000 as training and
between 1990 to 2000 for the test dataset. For KB, Freebase has been chosen and
only those relations have been included to $\mathcal{F}$ that both left hand
side and right hand side of relation is available in a sentence in the corpus.
If for both NEs' of a sentence there was no relation in Freebase we have used
\textit{NA} relation and we call it a negative instance. We used a subset of
this dataset.
For training we picked 8409 positive and the same number of negative instances.
For test set we used 4000 instances, balanced between number of positive and
negative instances. Based on this dataset we run our experiments. As an example
you can see one positive and one negative instance below:
\begin{description}
\item[POSITIVE]\hfil \\ \textit{left entity:Edward M. Liddy ~~  right
entity:Allstate ~~ trigger:executive ~~ path:appos|->appos->executive->prep->of->pobj->|pobj
~~ \\ Freebase-Rel:/business/company\_shareholder/major\_shareholder\_of
~~ NE:(PERSON, ORGANIZATION) }
\item[NEGATIVE] \hfil \\ \textit{left entity: Susan Arnold ~~ right entity:
Sweethearts ~~ trigger: producer        NE:(PERSON,MISC)
~~ path: appos|->appos->producer->dep->|dep   ~~    Freebase-Rel:NA}
\end{description}

\subsection{Experiments}
\label{ssec:textkb-exp-exp}


We arrange different settings and experiments to examine the role of our
features and compare it to the setting previously used in \cite{Bordes2011} and
\cite{Bordes2012}. We start from features used in the previous work and
gradually introduce our own features in each experiment.

\begin {description}
\item[KB~(Experiment 1)] \hfil \\
In this experiment we only use Freebase relations for the sake of repeating the
proposed setting in \cite{Bordes2011}. In this setting, there is no actual link
from text to KB and therefor by comparing other experiments against experiment 1
we can see the efftect of linking through our proposed formalism.
\item[KB+Trigger~(Experiment 2)] \hfill \\
This experiment is designed to see the effect of adding surface pattern features
to Freebase relations and learn them jointly. In this settings, our train
dataset consists only of Freebase relations and trigger relations.
\item[Text+KB\char`\\Trigger ~(Experiment 3)] \hfill \\
All the features are used in this experiment except
the surface pattern (trigger) relations. 
\item[Text+KB~(Experiment 4)] \hfill \\
All the features including surface patterns are employed and we have full
linkage between corpus and KB. 
\end{description}

All of these experiments are conducted with same parameters of the model. Even
though the structure of the forth experiment is more complex than for example
the first experiment but by using same parameters the outcome of the experiments
can be analyzed based on the effectiveness of the feature set of each
experiments. For all experiments we used 50-dimensional embeddings and we used
SME-Bil neural distributed model. Number of iterations were 500 for all
experiments and with fixed number of batches, namely 100. The details of this
model has been previously described in \autoref{??}.

In the next section we will see the results and comparison of different
experiments.


\subsection{Evaluation}
\label{ssec:textkb-exp-exp}

The goal our model is to predict a relation between
two given entities. So how do we do prediction with our model? We basically rank
all possible relations based on a similarity score we already introduced in
\eqref{eq:dot}. Given two entities, we take their embeddings we learned through
the training phase and then iterate over all embeddings of Freebase ( or
surface patterns) and calculate a score for triple of these three embeddings.
Then we can sort all relations based on this score, this will construct our
prediction. One should notice that the output is not probabilistic. In
\cite{Bordes2011} a method based on kernel density estimation has been proposed
to make a probabilistic output but since it is reported ot overfit on the
training dataset I don't use it here. 
Having all the relations sorted for a given tuple of entities for our test
cases, we can find the predicted rank of the true relation and report statistics
per relation and for whole test set. The statistics I report here are mean and
median of true rank pre relation and also the number of times that true relation
appeared on top-$r$ relations, here 100. For aggregating these statistics for
the whole dataset we can follow to protocols. In one type of aggregation we
give more weight for more frequent relations (Micro statistics) and in the other
protocol we compute mean and median of true ranks without considering the
popularity of a relation, we give equal weight for each relation (Macro
statistics).

\begin{table}[ht]
\caption{Relation Prediction Evaluation  }

\label{tbl:rel-pred} % title name of the table
\centering % 
\begin{tabular}{l  l c  c c}

\hline\hline % inserting double-line
 Dataset &  Feature Type &  & Micro & Macro
\\ [0.5ex] 
\hline % inserts single-line

  &   & mean & 71.11 & 78.39 \\[-1ex]
   &  & median & 31.0 & 72.73 \\[-1ex]
\raisebox{1.0ex}{Experiment 1} &  \raisebox{1.0ex}{KB}
 &r@100& 67.05 & 62.79 \\[1ex]

&   & mean & 639.67 & 534.72 \\[-1ex]
   &  & median & 544.0 & 503.20 \\[-1ex]
\raisebox{1.0ex}{Experiment 2} &  \raisebox{1.0ex}{KB+Trigger}
 &r@100& 20.92 & 19.85 \\[1ex]
 
 &   & mean & 59.63 & 61.59 \\[-1ex]
   &  & median & 25.50 & 56.49 \\[-1ex]
\raisebox{1.0ex}{Experiment 3} &  \raisebox{1.0ex}{All \textbackslash{Trigger}}
 &r@100& 73.85 & 73.88 \\[1ex]
 
 &   & mean & 6.72 & 57.13 \\[-1ex]
   &  & median & 2.0 & 55.71 \\[-1ex]
\raisebox{1.0ex}{Experiment 4} &  \raisebox{1.0ex}{All}
 &r@100& 98.85 & 76.88 \\[1ex]
 
 
 
\hline % inserts single-line
\end{tabular}

\end{table}

the first observation from \autoref{tbl:rel-pred} is the significant role of
features. We can see that the \textit{Experiment 4}  has the best
performance in all the statistics and it benefits from full set of features. The
comparison between results of the first model, Bordes plain \textit{KB} model
and the \textit{KB+Text} model reveals that introduced features are highly
effective and the reconstruction (extending) of KBs can be done better. Not that
with this formalization we can achieve a relation extraction model linked to
text but also we can use it as a model to extend and populate current KBs. The
big difference between Micro and Macro statistics for this model suggests that
its good performance on frequent relations can also be achieved  for less
frequent relations through learning larger size of Freebase. We believe all the
models will for sure benefit from this but the forth model has already shown a
superior performance for relations occurred more than a certain threshold and by
feeding bigger portions of our KB we can help less frequent relations to pass
this threshold. To investigate this effect we compare the per relation
performance for two group of high and low frequent relations. 


\begin{table}[ht]
\caption{Per Relation Evaluation  }

\label{tbl:rel-high-perf} % title name of the table
\centering % 
\begin{tabular}{l  c c c c  c c}

\hline\hline % inserting double-line
 Relation &  Frequency & &KB & KB+Trigger  & Text+KB\char`\\Trigger & Text+KB 
\\ [0.5ex] 
\hline % inserts single-line

  &   & mean & 88.65 & 789.27 & 83.27 & 2.55 \\[-1ex]
   &  & median & 60.0 & 797.0 &64.00 & 1.0\\[-1ex]
\raisebox{0.0ex}{NA} &  2000
 &r@100& 61.55 & 14.10 & 62.79 & 99.95 \\[1ex]
 
   &   & mean & 49.37 & 577.79 & 28.85 & 3.60 \\[-1ex]
   &  & median & 9.0 & 468.00 &5.00 & 2.00\\[-1ex]
\raisebox{0.0ex}{/location/containedby} &  688
 &r@100& 78.34 & 20.34 & 89.39 & 99.85 \\[1ex]
 
  &   & mean & 39.03 & 531.25 & 25.09 & 6.68 \\[-1ex]
   &  & median & 12.50 & 410.5 &8.0 & 4.0\\[-1ex]
\raisebox{0.0ex}{/people/person/place\_lived} &  132
 &r@100& 84.84 & 25.75 & 93.18 & 100.0 \\[1ex]

  &   & mean & 47.00 & 359.48  & 16.43 & 2.83 \\[-1ex]
   &  & median & 7.5 & 105.5 &3.0 & 2.0\\[-1ex]
\raisebox{0.0ex}{/person/company} &  124
 &r@100& 79.83 & 50 & 95.16 & 100.0 \\[1ex]


 
   &   & mean & 47.95 & 433.36 & 15.60 & 3.92 \\[-1ex]
   &  & median & 15.0 & 315.50 & 4.0 & 2.0\\[-1ex]
\raisebox{0.0ex}{/deceased\_person/place\_of\_death} &  80
 &r@100& 78.75 & 36.25 & 95.00 & 100.0 \\[1ex]
 
\hline % inserts single-line
\end{tabular}

\end{table}