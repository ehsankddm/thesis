\chapter{Machine Learning}

\section{Representation Learning}
\label {ch:repr-learning}

In this chapter, we will define and justify the task of
\emph{Representation Learning} and we will see different families of methods for
inducing word representation and its application in NLP.

In machine learning specially in industry, most of the labor is dedicated to
\emph{Feature Engineering}. Extracting informative features is the crucial part
of most supervised methods and it is done mostly manually. While many different
applications share common learning models and classifiers, the difference in
performance of competing methods mostly goes to the data representation and
hand-crafted features that they use. This observation reveals an important
weakness in current models, namely their inability to extract and organize
discriminative features from data. Representation learning is an umberella term
for a family of unsupervised methods to learn features from data. Most of recent
works on the application of this idea in NLP focus on inducing word
representations. \emph{Word representation} is a mathematical object, usually a
vector, which each dimension in this vector represents a grammatical or
semantical feature to identify this word and is induced automatically from data
\cite{Turian2010b}. Recently, it has been shown in \cite{Turian2010b} and \cite{Collobert2011} that using
 induced feautres can be helpful to improve state-of-the-art methods in 
different NLP tasks. It seems that relation extraction can also benefit from such features since similar tasks like 
semantic role labeling has been shown to benefit from
induced word representations. In section ~\ref{sec:contr} we will describe one
possible way of incorporating this idea in to our task. In the next two sections, two major
families of representations will be shortly reviewed.


\subsection{Distributional Representation}
\label{sec:distl-repr}
In distributional semantics, the meaning of a word is expressed by the context
that it appears in it \cite{Harris1981}. Features that are used to represent the
meaning of a word are other words in its neighborhood as it is so called the
context. In some approaches like LDA and latent semantic analysis (LSA), 
the context is defined in the scope of a document rather than a window around a
word. To represent word meanings in via distributional approach, one should
start from count matrix (or zero-one co-occurence matrix) which each row
represents a word and each column is a context. The representation can be
limited to raw usage of the very same matrix or some transforms like
\emph{tf-idf} will be applied first. A further analysis over this matrix to
extract more meaningful features is applying dimensionality reduction methods or
clustering models to induce latent distributional representations. A similar
clustering method to k-means is used in \cite{Lin2009} to represent phrase and
word meanings and brown clustering algorithm \cite{Brown1992} has been shown to
have impact on near to state-of-the-art NLP tasks \cite{Turian2010b}. 


\subsection{Distributed Representation}
\label{sec:disted-repr}
Distributed representation has been introduced in the literature for the first
time in \cite{Bengio2003} where Bengio et al. introduced a first language
model based on deep learning methods\cite{Bengio2009b}. Deep learning is
learning through several layers of neural networks which each layer is
responsible to learn a different concept and each concecpt is built over other
more abstract concepts. In the deep learning society, any word representation
that is induced with a neural network is called \emph{Word Embedding}. 
In contrast to raw count matrix in distributional representations, word embeddings are low-dimensional, dense and real-valued vectors.
 The term, \textbf{`Distributed'}, in this context refers to the fact that
 exponential number of objects (clusters) can be modeled by word embeddings.
 Here we will see two famous models to induce for such representations. One
 family will use n-grams to learn word representation jointly with a language
 model and the other family learns the embedding from structured resources.
In \cite{Collobert2008a}, Weston and Collobert use a non-probabilistic and
discriminative model to jointly learn word embeddings and a language model that
can separate plausible n-grams from noisy ones. For each word in a n-gram, they
combine the word embeddings and use it as positive example. They put noise in
the n-gram to make negative examples and then train a neural network to learn to
classify postive labels from negative ones. The parameters of neural network
(neural language model) and word embedding values will be learned jointly by an
optimization method called \emph{Stochastic Gradient Descent} \cite{Bottou2010}.

A hierarchical dsitributed language model (HLBL) proposed by Mnih and
Hinton in \cite{Mnih2009} is another influential work on word embeddings. In
this model a probabilistic linear neural network(LBL) will be trained to 
combine word embeddings in first $n-1$ words of a n-gram to predict the $n_th$ word.

Weston-Collobert model and HLBL by Mnih and Hinton are evaluated in
\cite{Turian2010b} in two NLP tasks: chunking and named entity recognition. With
using word embeddings from these models combined with hand-crafted features, the
performance of both tasks are shown to be improved.
 
\subsection{Representation Learning of Knowledge Bases}
\label{sec: repr-learning-kb}
Bordes et al. in \cite{Bordes2011} and \cite{Bordes2012} have attempted to use
a neural distributed model to induce word representations from lexical resources
such as WordNet and knowledge bases (KB) like Freebase. In Freebase for example, each named entity is related
to another entity by an instance of a specific type of relation. In
\cite{Bordes2011}, each entity is represented as a vector and each relation is decomposed to two
matrices. Each of these matrices transform left and right-hand-side entities
to a semantic space. Similarity of transformed entities indicates that the
relation holds between the entities.  A prediction task is defined to evaluate
the embeddings. Given a relation and one of the entities, the task is to predict
the missing entity. The high accuracy (99.2\%) of the model on prediciton
of training data shows that learned representation highly captures attributes of
the entities and relations in Freebase.

Two major models are proposed in [Bordes2011] and [Bordes2012] to learn features in continues vector space
 from a Knowledge Bases(KB) which information is usually representated in form of triples of $(e_{i},r_{k} , e_{j} )$ 
 where $e_{i}$ and $e_{j}$ are $i_{th}$ and $j_{th}$ entities related
 by a binary relation of type $r_{k}$. The purpose of the models is to induce a vector space and associate
  each entity or relation to an embedding vector or a matrix.
  The dimensions of such an embedding vector are supposed to reflect a set of informative features of entities and relations.
   
   In the first model, \textbf{structured embeddings(SE)}, entities are modeled as \textit{d}-dimensional vectors.
    An associated vector to the $i_{th}$ entity, $e_{i}$, is $E_{i} \in \mathbb{R}^{d}$. Each relation $r_{k}$  
    is decomposed to two operators each represented as $d \times d$ matrix, $ R_k = (R_{k}^{left}, R_{k}^{right})$. 
    These operators transform the left and right entities to a new space induced by each relation and by using 
    a $p$-norm measure  (L1 norm in this work) they associate a similarity value or a score to each triple. 
    This similarity value is being calculated by
    Equation ~\eqref{eq:sim}. 
    \begin{equation}
    \label{eq:sim}Sim(E_{i}, E_{j}, R) = ||R_{k}^{left}E_{i} - R_{k}^{right}E_{j} ||_{1}
    \end{equation}
    The similarity between transformed entities works as a score to measure the strength of a relation holds between two entities. 
      
    Using the idea of contrastive learning , the model will be trained to increase similarity of 
    embeddings for a positive triple (a triple which exists in the KB) or lowering its rank among other training samples
    and decrease the similarity of embeddings when the relation doesn't hold (negative triple) or raising its rank . 
    For each positive triple, two negative triples will be generated by randomly alternating the right entity or left entity with other entities.
    Inspired from large margin methods a constraint is introdcued on the model that forces 
    negative triples to have lower associated similarity value  than correspondent positive triples by a large margin.
    
    The second model, \textbf{Semantic Matching Energy using Bilinear layers(SME-Bil)}, 
    is using a different represention for relations,weighted bilinear transformation of embeddings 
    and  dot product similarity function instead of L1 norm. 
    In this model, each relation is represented by a \textit{d}-dimensional vector $R_{k}$ same as entities. 
    For triple $(e_{i},r_{k} , e_{j} )$ , the model combines the weighted transformation of each entity embedding with 
    the weighted embedding of relation using element-wise vector product. as it is shown in Equation ~\eqref{eq:bil}.
    \begin{equation}
    \label{eq:bil} E'_{left} = (W_{i} E_{i}) \odot (W_{k} R_{k}) + b_{left}
    \end{equation}
    $W_{i}$ and $W_{k}$ are $d \times d$ weight matrices and $b_{left}$ is a $d$-dimensional bias vector. 
    The same equation holds for transforming the right entity embeddings to $E'_{right}$. Finally, the associated score for the triple
     can be calucated by dot product of $E'_{left}$ and $E'_{right}$ which is shown in Equation ~\ref{eq:dot}.
    
   \begin{equation}
    \label{eq:dot} Sim(E_{i}, E_{j}, R_{k}) = -E'_{left}E'_{right}
   \end{equation}
    Similar constraints to the first model are also applied to this model and 
     both models can be trained by stochahstic gradient descent (SGD) which we
     introduced in \autoref{sec:SGD}.



    